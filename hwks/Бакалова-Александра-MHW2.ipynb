{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d30c3c0",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c3e38",
   "metadata": {},
   "source": [
    "## First, install the dependenices and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4377d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in ./.local/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: python-slugify in ./.local/lib/python3.8/site-packages (from kaggle) (8.0.0)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from kaggle) (2.7.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./.local/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "ln: failed to create symbolic link '/usr/bin/kaggle': File exists\n",
      "Hit:1 http://mirror.yandex.ru/ubuntu focal InRelease\n",
      "Get:2 http://mirror.yandex.ru/ubuntu focal-updates InRelease [114 kB]          \n",
      "Get:3 http://mirror.yandex.ru/ubuntu focal-backports InRelease [108 kB]        \n",
      "Hit:4 http://storage.yandexcloud.net/dataproc/ci/trunk/80-d30331cee3848e39 focal InRelease\n",
      "Get:5 http://mirror.yandex.ru/mirrors/postgresql focal-pgdg InRelease [91.6 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Get:7 https://repos.influxdata.com/ubuntu focal InRelease [7,019 B]            \n",
      "Hit:8 https://repo.saltproject.io/py3/ubuntu/20.04/amd64/3002 focal InRelease  \n",
      "Get:9 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 Packages [2,372 kB]\n",
      "Get:10 http://mirror.yandex.ru/ubuntu focal-updates/main i386 Packages [785 kB]\n",
      "Get:11 http://mirror.yandex.ru/ubuntu focal-updates/main Translation-en [408 kB]\n",
      "Get:12 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 c-n-f Metadata [16.3 kB]\n",
      "Get:13 http://mirror.yandex.ru/ubuntu focal-updates/restricted i386 Packages [30.4 kB]\n",
      "Get:14 http://mirror.yandex.ru/ubuntu focal-updates/restricted amd64 Packages [1,598 kB]\n",
      "Get:15 http://mirror.yandex.ru/ubuntu focal-updates/restricted Translation-en [225 kB]\n",
      "Get:16 http://mirror.yandex.ru/ubuntu focal-updates/restricted amd64 c-n-f Metadata [620 B]\n",
      "Get:17 http://mirror.yandex.ru/ubuntu focal-updates/universe amd64 Packages [1,024 kB]\n",
      "Get:18 http://mirror.yandex.ru/ubuntu focal-updates/universe i386 Packages [710 kB]\n",
      "Get:19 http://mirror.yandex.ru/ubuntu focal-updates/universe Translation-en [237 kB]\n",
      "Get:20 http://mirror.yandex.ru/ubuntu focal-updates/universe amd64 c-n-f Metadata [23.5 kB]\n",
      "Get:21 http://mirror.yandex.ru/ubuntu focal-backports/universe amd64 Packages [24.9 kB]\n",
      "Get:22 http://mirror.yandex.ru/ubuntu focal-backports/universe i386 Packages [13.8 kB]\n",
      "Get:23 http://mirror.yandex.ru/ubuntu focal-backports/universe amd64 c-n-f Metadata [880 B]\n",
      "Get:24 http://mirror.yandex.ru/mirrors/postgresql focal-pgdg/main amd64 Packages [259 kB]\n",
      "Err:7 https://repos.influxdata.com/ubuntu focal InRelease                      \n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY D8FF8E1F7DF8B07E\n",
      "Get:25 https://packages.fluentbit.io/ubuntu/focal focal InRelease [7,564 B]\n",
      "Get:26 http://security.ubuntu.com/ubuntu focal-security/main i386 Packages [556 kB]\n",
      "Get:27 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1,993 kB]\n",
      "Get:28 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [326 kB]\n",
      "Get:29 http://security.ubuntu.com/ubuntu focal-security/main amd64 c-n-f Metadata [12.2 kB]\n",
      "Get:30 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1,495 kB]\n",
      "Get:31 http://security.ubuntu.com/ubuntu focal-security/restricted i386 Packages [29.1 kB]\n",
      "Get:32 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [211 kB]\n",
      "Get:33 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 c-n-f Metadata [624 B]\n",
      "Get:34 http://security.ubuntu.com/ubuntu focal-security/universe i386 Packages [579 kB]\n",
      "Get:35 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [794 kB]\n",
      "Get:36 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [154 kB]\n",
      "Get:37 https://packages.fluentbit.io/ubuntu/focal focal/main amd64 Packages [24.6 kB]\n",
      "Get:38 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [16.9 kB]\n",
      "Get:39 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [22.9 kB]\n",
      "Get:40 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [5,488 B]\n",
      "Get:41 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [528 B]\n",
      "Fetched 14.4 MB in 3s (4,925 kB/s)                         \n",
      "Reading package lists... Done\n",
      "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repos.influxdata.com/ubuntu focal InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY D8FF8E1F7DF8B07E\n",
      "N: Skipping acquire of configured file 'main/binary-i386/Packages' as repository 'https://packages.fluentbit.io/ubuntu/focal focal InRelease' doesn't support architecture 'i386'\n",
      "W: Failed to fetch https://repos.influxdata.com/ubuntu/dists/focal/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY D8FF8E1F7DF8B07E\n",
      "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  p7zip\n",
      "Suggested packages:\n",
      "  p7zip-rar\n",
      "The following NEW packages will be installed:\n",
      "  p7zip p7zip-full\n",
      "0 upgraded, 2 newly installed, 0 to remove and 56 not upgraded.\n",
      "Need to get 1,545 kB of archives.\n",
      "After this operation, 5,896 kB of additional disk space will be used.\n",
      "Get:1 http://mirror.yandex.ru/ubuntu focal/universe amd64 p7zip amd64 16.02+dfsg-7build1 [358 kB]\n",
      "Get:2 http://mirror.yandex.ru/ubuntu focal/universe amd64 p7zip-full amd64 16.02+dfsg-7build1 [1,187 kB]\n",
      "Fetched 1,545 kB in 0s (15.8 MB/s)    \n",
      "Selecting previously unselected package p7zip.\n",
      "(Reading database ... 126748 files and directories currently installed.)\n",
      "Preparing to unpack .../p7zip_16.02+dfsg-7build1_amd64.deb ...\n",
      "Unpacking p7zip (16.02+dfsg-7build1) ...\n",
      "Selecting previously unselected package p7zip-full.\n",
      "Preparing to unpack .../p7zip-full_16.02+dfsg-7build1_amd64.deb ...\n",
      "Unpacking p7zip-full (16.02+dfsg-7build1) ...\n",
      "Setting up p7zip (16.02+dfsg-7build1) ...\n",
      "Setting up p7zip-full (16.02+dfsg-7build1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle\n",
    "! sudo ln -s ~/.local/bin/kaggle /usr/bin/kaggle\n",
    "! sudo apt-get update && sudo apt-get install p7zip-full -y\n",
    "! sudo pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab519ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n",
      "Downloading VisitsStream.tsv.7z to /home/ubuntu\n",
      "100%|█████████████████████████████████████▉| 1.98G/1.99G [00:40<00:00, 55.0MB/s]\n",
      "100%|██████████████████████████████████████| 1.99G/1.99G [00:40<00:00, 52.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle competitions download -c avito-context-ad-clicks -f VisitsStream.tsv.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85586871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel Xeon Processor (Icelake) (606A0),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 2132222657 bytes (2034 MiB)\n",
      "\n",
      "Extracting archive: VisitsStream.tsv.7z\n",
      "--\n",
      "Path = VisitsStream.tsv.7z\n",
      "Type = 7z\n",
      "Physical Size = 2132222657\n",
      "Headers Size = 136\n",
      "Method = LZMA:24\n",
      "Solid = -\n",
      "Blocks = 1\n",
      "\n",
      "      0% - VisitsStream.ts                        1% - VisitsStream.ts                        2% - VisitsStream.ts                        3% - VisitsStream.ts                        4% - VisitsStream.ts                        5% - VisitsStream.ts                        6% - VisitsStream.ts                        7% - VisitsStream.ts                        8% - VisitsStream.ts                        9% - VisitsStream.ts                       10% - VisitsStream.ts                       11% - VisitsStream.ts                       12% - VisitsStream.ts                       13% - VisitsStream.ts                       14% - VisitsStream.ts                       15% - VisitsStream.ts                       16% - VisitsStream.ts                       17% - VisitsStream.ts                       18% - VisitsStream.ts                       19% - VisitsStream.ts                       20% - VisitsStream.ts                       21% - VisitsStream.ts                       22% - VisitsStream.ts                       23% - VisitsStream.ts                       24% - VisitsStream.ts                       25% - VisitsStream.ts                       26% - VisitsStream.ts                       27% - VisitsStream.ts                       28% - VisitsStream.ts                       29% - VisitsStream.ts                       30% - VisitsStream.ts                       31% - VisitsStream.ts                       32% - VisitsStream.ts                       33% - VisitsStream.ts                       34% - VisitsStream.ts                       35% - VisitsStream.ts                       36% - VisitsStream.ts                       37% - VisitsStream.ts                       38% - VisitsStream.ts                       39% - VisitsStream.ts                       40% - VisitsStream.ts                       41% - VisitsStream.ts                       42% - VisitsStream.ts                       43% - VisitsStream.ts                       44% - VisitsStream.ts                       45% - VisitsStream.ts                       46% - VisitsStream.ts                       47% - VisitsStream.ts                       48% - VisitsStream.ts                       49% - VisitsStream.ts                       50% - VisitsStream.ts                       51% - VisitsStream.ts                       52% - VisitsStream.ts                       53% - VisitsStream.ts                       54% - VisitsStream.ts                       55% - VisitsStream.ts                       56% - VisitsStream.ts                       57% - VisitsStream.ts                       58% - VisitsStream.ts                       59% - VisitsStream.ts                       60% - VisitsStream.ts                       61% - VisitsStream.ts                       62% - VisitsStream.ts                       63% - VisitsStream.ts                       64% - VisitsStream.ts                       65% - VisitsStream.ts                       66% - VisitsStream.ts                       67% - VisitsStream.ts                       68% - VisitsStream.ts                       69% - VisitsStream.ts                       70% - VisitsStream.ts                       71% - VisitsStream.ts                       72% - VisitsStream.ts                       73% - VisitsStream.ts                       74% - VisitsStream.ts                       75% - VisitsStream.ts                       76% - VisitsStream.ts                       77% - VisitsStream.ts                       78% - VisitsStream.ts                       79% - VisitsStream.ts                       80% - VisitsStream.ts                       81% - VisitsStream.ts                       82% - VisitsStream.ts                       83% - VisitsStream.ts                       84% - VisitsStream.ts                       85% - VisitsStream.ts                       86% - VisitsStream.ts                       87% - VisitsStream.ts                       88% - VisitsStream.ts                       89% - VisitsStream.ts                       90% - VisitsStream.ts                       91% - VisitsStream.ts                       92% - VisitsStream.ts                       93% - VisitsStream.ts                       94% - VisitsStream.ts                       95% - VisitsStream.ts                       96% - VisitsStream.ts                       97% - VisitsStream.ts                       98% - VisitsStream.ts                       99% - VisitsStream.ts                      100%    Everything is Ok\n",
      "\n",
      "Size:       13180996392\n",
      "Compressed: 2132222657\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p filefolder && 7z x VisitsStream.tsv.7z -ofilefolder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e44d1a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisitsStream.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! ls filefolder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b593436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID\tIPID\tAdID\tViewDate\r\n",
      "59703\t1259356\t469877\t2015-04-25 00:00:00.0\r\n",
      "154389\t1846749\t27252551\t2015-04-25 00:00:00.0\r\n",
      "218628\t2108380\t31685325\t2015-04-25 00:00:00.0\r\n",
      "231535\t837110\t18827716\t2015-04-25 00:00:00.0\r\n",
      "282306\t1654210\t29363673\t2015-04-25 00:00:00.0\r\n",
      "295068\t601505\t588324\t2015-04-25 00:00:00.0\r\n",
      "501897\t158476\t4103261\t2015-04-25 00:00:00.0\r\n",
      "655394\t631692\t9860544\t2015-04-25 00:00:00.0\r\n",
      "765603\t804403\t29475627\t2015-04-25 00:00:00.0\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! cat filefolder/VisitsStream.tsv | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f679de",
   "metadata": {},
   "source": [
    "## Now, write MapReduce code and test it on subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "394cdfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! cat filefolder/VisitsStream.tsv | head -n 10000 > testfile.tsv\n",
    "! sed -i -e '1'd testfile.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53a5792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest_session.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest_session.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def tsv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''), delimiter=\"\\t\")\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in tsv_stream():\n",
    "        userid = row[0]\n",
    "        date_str = row[3]\n",
    "        print(\"{}+{}\\t\".format(userid, date_str))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    user_date, _ = next(sys.stdin).split(\"\\t\")\n",
    "    user, date_str = user_date.split(\"+\")\n",
    "    last_date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    duration = 0\n",
    "    max_duration = 0\n",
    "    for current_user_date, _ in kv_stream(\"\\t\"):\n",
    "        current_user, current_date_str = current_user_date.split(\"+\")\n",
    "        current_date = datetime.strptime(current_date_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        \n",
    "        if current_user != user:\n",
    "            print(\"{}\\t{}\".format(user, max_duration))\n",
    "            user = current_user\n",
    "            last_date = current_date\n",
    "            max_duration = 0\n",
    "            duration = 0\n",
    "        else:\n",
    "            if (current_date - last_date).seconds <= 15 * 60:\n",
    "                duration += (current_date - last_date).seconds\n",
    "            else:\n",
    "                duration = 0\n",
    "            max_duration = max(max_duration, duration)\n",
    "            last_date = current_date\n",
    "    max_duration = max(max_duration, duration)\n",
    "    print(\"{}\\t{}\".format(user, max_duration))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1cdff0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    first_10_stream = islice(kv_stream(\"+\"), 10)\n",
    "    \n",
    "    for user, duration in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(user, duration.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "894a0423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 9999/9999 [00:00<00:00, 661329.09it/s]\n",
      "CPU times: user 623 µs, sys: 8.33 ms, total: 8.96 ms\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! cat testfile.tsv | \\\n",
    "    tqdm --total $(cat testfile.tsv | wc -l) | \\\n",
    "    python longest_session.py map | \\\n",
    "    sort -t'+' -k1,1 -k2,2 | \\\n",
    "    python longest_session.py reduce > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "434237f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 5820/5820 [00:00<00:00, 1619723.26it/s]\n",
      "CPU times: user 4.09 ms, sys: 3.14 ms, total: 7.23 ms\n",
      "Wall time: 186 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! cat result.txt | \\\n",
    "    tqdm --total $(cat result.txt | wc -l) | \\\n",
    "    python top10.py map | \\\n",
    "    sort -t'+' -k2,2nr -k1,1 | \\\n",
    "    python top10.py reduce > top-10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9019307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147902\t160\r\n",
      "365809\t159\r\n",
      "1316443\t158\r\n",
      "2560649\t158\r\n",
      "3093941\t158\r\n",
      "3783187\t157\r\n",
      "1435568\t156\r\n",
      "4285777\t156\r\n",
      "444510\t156\r\n",
      "218628\t155\r\n"
     ]
    }
   ],
   "source": [
    "! cat top-10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7d8ab",
   "metadata": {},
   "source": [
    "## Now, run the code on hdfs on the same subset of data.\n",
    "Make sure that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c1cf8724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwx------   - mapred hadoop          0 2023-02-10 10:58 /hadoop\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2023-02-10 10:58 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2023-02-10 10:58 /user\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2023-02-10 10:58 /var\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "84fcfe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/hw2/data': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/hw2/data\n",
    "! hdfs dfs -mkdir -p /user/hw2/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d9efdf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/hw2/data/testfile.tsv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put testfile.tsv /user/hw2/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "40dc12e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-10 16:19:53,039 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8020]\n",
      "2023-02-10 16:19:53,044 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2023-02-10 16:19:53,045 INFO balancer.Balancer: included nodes = []\n",
      "2023-02-10 16:19:53,045 INFO balancer.Balancer: excluded nodes = []\n",
      "2023-02-10 16:19:53,045 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2023-02-10 16:19:53,051 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2023-02-10 16:19:54,239 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2023-02-10 16:19:54,239 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2023-02-10 16:19:54,240 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2023-02-10 16:19:54,240 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2023-02-10 16:19:54,240 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2023-02-10 16:19:54,240 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2023-02-10 16:19:54,240 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2023-02-10 16:19:54,240 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2023-02-10 16:19:54,247 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2023-02-10 16:19:54,247 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2023-02-10 16:19:54,269 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.27:9866\n",
      "2023-02-10 16:19:54,270 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.26:9866\n",
      "2023-02-10 16:19:54,272 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2023-02-10 16:19:54,272 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Feb 10, 2023 4:19:54 PM           0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8020\n",
      "Feb 10, 2023 4:19:54 PM  Balancing took 1.453 seconds\n"
     ]
    }
   ],
   "source": [
    "! sudo mkdir -p /usr/lib/hadoop/logs\n",
    "! sudo chmod 0777 /usr/lib/hadoop/logs\n",
    "! sudo -u hdfs hdfs balancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "323bc89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\r\n"
     ]
    }
   ],
   "source": [
    "! sudo find /usr/ -name hadoop-streaming.jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "31cacafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar: symbolic link to hadoop-streaming-3.2.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "! sudo file /usr/lib/hadoop-mapreduce/hadoop-streaming.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "23e3cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/hw2/result': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob8625384959581521654.jar tmpDir=null\n",
      "2023-02-10 16:23:48,728 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-10 16:23:49,068 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-10 16:23:49,121 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-10 16:23:49,122 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-10 16:23:49,417 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1676026733147_0001\n",
      "2023-02-10 16:23:49,746 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-02-10 16:23:49,871 INFO mapreduce.JobSubmitter: number of splits:20\n",
      "2023-02-10 16:23:50,282 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1676026733147_0001\n",
      "2023-02-10 16:23:50,284 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-10 16:23:50,535 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-02-10 16:23:50,536 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-02-10 16:23:51,107 INFO impl.YarnClientImpl: Submitted application application_1676026733147_0001\n",
      "2023-02-10 16:23:51,175 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8088/proxy/application_1676026733147_0001/\n",
      "2023-02-10 16:23:51,177 INFO mapreduce.Job: Running job: job_1676026733147_0001\n",
      "2023-02-10 16:23:58,290 INFO mapreduce.Job: Job job_1676026733147_0001 running in uber mode : false\n",
      "2023-02-10 16:23:58,291 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-10 16:24:05,355 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2023-02-10 16:24:07,365 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2023-02-10 16:24:08,371 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-02-10 16:24:09,376 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "2023-02-10 16:24:14,406 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "2023-02-10 16:24:15,411 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2023-02-10 16:24:19,428 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2023-02-10 16:24:20,434 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "2023-02-10 16:24:23,445 INFO mapreduce.Job:  map 80% reduce 9%\n",
      "2023-02-10 16:24:24,449 INFO mapreduce.Job:  map 80% reduce 18%\n",
      "2023-02-10 16:24:25,454 INFO mapreduce.Job:  map 85% reduce 18%\n",
      "2023-02-10 16:24:26,460 INFO mapreduce.Job:  map 100% reduce 51%\n",
      "2023-02-10 16:24:27,464 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-10 16:24:28,477 INFO mapreduce.Job: Job job_1676026733147_0001 completed successfully\n",
      "2023-02-10 16:24:28,561 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=58712\n",
      "\t\tFILE: Number of bytes written=5680059\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2309617\n",
      "\t\tHDFS: Number of bytes written=80722\n",
      "\t\tHDFS: Number of read operations=75\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=20\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=16\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=290364\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=132951\n",
      "\t\tTotal time spent by all map tasks (ms)=96788\n",
      "\t\tTotal time spent by all reduce tasks (ms)=44317\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=96788\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=44317\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=297332736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=136141824\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=9999\n",
      "\t\tMap output records=9999\n",
      "\t\tMap output bytes=307462\n",
      "\t\tMap output materialized bytes=63834\n",
      "\t\tInput split bytes=2760\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9980\n",
      "\t\tReduce shuffle bytes=63834\n",
      "\t\tReduce input records=9999\n",
      "\t\tReduce output records=8105\n",
      "\t\tSpilled Records=19998\n",
      "\t\tShuffled Maps =60\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=60\n",
      "\t\tGC time elapsed (ms)=3187\n",
      "\t\tCPU time spent (ms)=17890\n",
      "\t\tPhysical memory (bytes) snapshot=7205294080\n",
      "\t\tVirtual memory (bytes) snapshot=99810271232\n",
      "\t\tTotal committed heap usage (bytes)=7136083968\n",
      "\t\tPeak Map Physical memory (bytes)=352260096\n",
      "\t\tPeak Map Virtual memory (bytes)=4341432320\n",
      "\t\tPeak Reduce Physical memory (bytes)=211361792\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4345131008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2306857\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=80722\n",
      "2023-02-10 16:24:28,562 INFO streaming.StreamJob: Output directory: /user/hw2/result/\n",
      "CPU times: user 720 ms, sys: 76.2 ms, total: 796 ms\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/hw2/result || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"longest-session\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/longest_session.py \\\n",
    "-mapper \"python3 longest_session.py map\" \\\n",
    "-reducer \"python3 longest_session.py reduce\" \\\n",
    "-input /user/hw2/data/ \\\n",
    "-output /user/hw2/result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1efd06f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/hw2/top10/': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob1610826423214658925.jar tmpDir=null\n",
      "2023-02-10 16:25:47,215 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-10 16:25:47,456 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-10 16:25:47,498 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-10 16:25:47,499 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-10 16:25:47,750 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1676026733147_0002\n",
      "2023-02-10 16:25:48,030 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2023-02-10 16:25:48,088 INFO mapreduce.JobSubmitter: number of splits:21\n",
      "2023-02-10 16:25:48,243 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1676026733147_0002\n",
      "2023-02-10 16:25:48,245 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-10 16:25:48,422 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-02-10 16:25:48,422 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-02-10 16:25:48,512 INFO impl.YarnClientImpl: Submitted application application_1676026733147_0002\n",
      "2023-02-10 16:25:48,544 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8088/proxy/application_1676026733147_0002/\n",
      "2023-02-10 16:25:48,546 INFO mapreduce.Job: Running job: job_1676026733147_0002\n",
      "2023-02-10 16:25:54,626 INFO mapreduce.Job: Job job_1676026733147_0002 running in uber mode : false\n",
      "2023-02-10 16:25:54,627 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-10 16:26:01,709 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "2023-02-10 16:26:03,724 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "2023-02-10 16:26:04,729 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-02-10 16:26:07,749 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2023-02-10 16:26:09,758 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "2023-02-10 16:26:13,781 INFO mapreduce.Job:  map 86% reduce 0%\n",
      "2023-02-10 16:26:15,791 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2023-02-10 16:26:16,795 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-10 16:26:17,799 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-10 16:26:17,805 INFO mapreduce.Job: Job job_1676026733147_0002 completed successfully\n",
      "2023-02-10 16:26:17,886 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=28052\n",
      "\t\tFILE: Number of bytes written=5388760\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=313684\n",
      "\t\tHDFS: Number of bytes written=119\n",
      "\t\tHDFS: Number of read operations=68\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=21\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=14\n",
      "\t\tRack-local map tasks=7\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=254448\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=40800\n",
      "\t\tTotal time spent by all map tasks (ms)=84816\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13600\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=84816\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13600\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=260554752\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=41779200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8105\n",
      "\t\tMap output records=8105\n",
      "\t\tMap output bytes=88827\n",
      "\t\tMap output materialized bytes=34024\n",
      "\t\tInput split bytes=2898\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6661\n",
      "\t\tReduce shuffle bytes=34024\n",
      "\t\tReduce input records=8105\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=16210\n",
      "\t\tShuffled Maps =21\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=21\n",
      "\t\tGC time elapsed (ms)=2741\n",
      "\t\tCPU time spent (ms)=16080\n",
      "\t\tPhysical memory (bytes) snapshot=7173099520\n",
      "\t\tVirtual memory (bytes) snapshot=95487262720\n",
      "\t\tTotal committed heap usage (bytes)=7028080640\n",
      "\t\tPeak Map Physical memory (bytes)=351084544\n",
      "\t\tPeak Map Virtual memory (bytes)=4345810944\n",
      "\t\tPeak Reduce Physical memory (bytes)=205381632\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4343869440\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=310786\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=119\n",
      "2023-02-10 16:26:17,887 INFO streaming.StreamJob: Output directory: /user/hw2/top10/\n",
      "CPU times: user 536 ms, sys: 93.3 ms, total: 629 ms\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/hw2/top10/\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python top10.py map\" \\\n",
    "-reducer \"python top10.py reduce\" \\\n",
    "-input /user/hw2/result/ \\\n",
    "-output /user/hw2/top10/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0294a4",
   "metadata": {},
   "source": [
    "## Fine! Now, we are ready to run the code on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "414d34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1'd filefolder/VisitsStream.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4f17c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hw2/data\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/hw2/data\n",
    "! hdfs dfs -mkdir -p /user/hw2/data\n",
    "! hdfs dfs -put filefolder/VisitsStream.tsv /user/hw2/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d8de3040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop 13180996366 2023-02-10 16:50 /user/hw2/data/VisitsStream.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/hw2/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ed3b6a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-10 17:05:02,845 INFO balancer.Balancer: Using a threshold of 1.0\n",
      "2023-02-10 17:05:02,847 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8020]\n",
      "2023-02-10 17:05:02,849 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 1.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2023-02-10 17:05:02,849 INFO balancer.Balancer: included nodes = []\n",
      "2023-02-10 17:05:02,849 INFO balancer.Balancer: excluded nodes = []\n",
      "2023-02-10 17:05:02,849 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2023-02-10 17:05:02,857 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2023-02-10 17:05:04,158 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2023-02-10 17:05:04,158 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2023-02-10 17:05:04,158 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2023-02-10 17:05:04,158 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2023-02-10 17:05:04,158 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2023-02-10 17:05:04,158 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2023-02-10 17:05:04,159 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2023-02-10 17:05:04,159 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2023-02-10 17:05:04,171 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2023-02-10 17:05:04,171 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2023-02-10 17:05:04,188 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.26:9866\n",
      "2023-02-10 17:05:04,188 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.27:9866\n",
      "2023-02-10 17:05:04,190 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2023-02-10 17:05:04,190 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Feb 10, 2023 5:05:04 PM           0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8020\n",
      "Feb 10, 2023 5:05:04 PM  Balancing took 1.574 seconds\n"
     ]
    }
   ],
   "source": [
    "! sudo -u hdfs hdfs balancer -threshold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bebbd2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 211162595328 (196.66 GB)\r\n",
      "Present Capacity: 177107763263 (164.94 GB)\r\n",
      "DFS Remaining: 163820646400 (152.57 GB)\r\n",
      "DFS Used: 13287116863 (12.37 GB)\r\n",
      "DFS Used%: 7.50%\r\n",
      "Replicated Blocks:\r\n",
      "\tUnder replicated blocks: 0\r\n",
      "\tBlocks with corrupt replicas: 0\r\n",
      "\tMissing blocks: 0\r\n",
      "\tMissing blocks (with replication factor 1): 0\r\n",
      "\tLow redundancy blocks with highest priority to recover: 0\r\n",
      "\tPending deletion blocks: 0\r\n",
      "Erasure Coded Block Groups: \r\n",
      "\tLow redundancy block groups: 0\r\n",
      "\tBlock groups with corrupt internal blocks: 0\r\n",
      "\tMissing block groups: 0\r\n",
      "\tLow redundancy blocks with highest priority to recover: 0\r\n",
      "\tPending deletion blocks: 0\r\n",
      "\r\n",
      "-------------------------------------------------\r\n",
      "Live datanodes (2):\r\n",
      "\r\n",
      "Name: 10.128.0.26:9866 (rc1a-dataproc-d-lcng4r2l7s9xard5.mdb.yandexcloud.net)\r\n",
      "Hostname: rc1a-dataproc-d-lcng4r2l7s9xard5.mdb.yandexcloud.net\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105581297664 (98.33 GB)\r\n",
      "DFS Used: 5682761728 (5.29 GB)\r\n",
      "Non DFS Used: 12662398976 (11.79 GB)\r\n",
      "DFS Remaining: 82871140352 (77.18 GB)\r\n",
      "DFS Used%: 5.38%\r\n",
      "DFS Remaining%: 78.49%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 1\r\n",
      "Last contact: Fri Feb 10 17:05:09 UTC 2023\r\n",
      "Last Block Report: Fri Feb 10 14:30:51 UTC 2023\r\n",
      "Num of Blocks: 29\r\n",
      "\r\n",
      "\r\n",
      "Name: 10.128.0.27:9866 (rc1a-dataproc-d-pvs2vuhaqfahjd4r.mdb.yandexcloud.net)\r\n",
      "Hostname: rc1a-dataproc-d-pvs2vuhaqfahjd4r.mdb.yandexcloud.net\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 105581297664 (98.33 GB)\r\n",
      "DFS Used: 7604355072 (7.08 GB)\r\n",
      "Non DFS Used: 12662431744 (11.79 GB)\r\n",
      "DFS Remaining: 80949514240 (75.39 GB)\r\n",
      "DFS Used%: 7.20%\r\n",
      "DFS Remaining%: 76.67%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 1\r\n",
      "Last contact: Fri Feb 10 17:05:09 UTC 2023\r\n",
      "Last Block Report: Fri Feb 10 16:47:42 UTC 2023\r\n",
      "Num of Blocks: 35\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! sudo -u hdfs hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54678331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hw2/result\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob5200837507345007401.jar tmpDir=null\n",
      "2023-02-10 17:05:51,152 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-10 17:05:51,636 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-10 17:05:51,693 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-10 17:05:51,694 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-10 17:05:52,032 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1676026733147_0003\n",
      "2023-02-10 17:05:52,372 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-02-10 17:05:52,462 INFO mapreduce.JobSubmitter: number of splits:50\n",
      "2023-02-10 17:05:52,877 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1676026733147_0003\n",
      "2023-02-10 17:05:52,879 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-10 17:05:53,150 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-02-10 17:05:53,150 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-02-10 17:05:53,232 INFO impl.YarnClientImpl: Submitted application application_1676026733147_0003\n",
      "2023-02-10 17:05:53,265 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8088/proxy/application_1676026733147_0003/\n",
      "2023-02-10 17:05:53,266 INFO mapreduce.Job: Running job: job_1676026733147_0003\n",
      "2023-02-10 17:05:58,338 INFO mapreduce.Job: Job job_1676026733147_0003 running in uber mode : false\n",
      "2023-02-10 17:05:58,338 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-10 17:06:16,477 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "2023-02-10 17:06:17,482 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "2023-02-10 17:06:22,505 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2023-02-10 17:06:29,538 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "2023-02-10 17:06:34,568 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2023-02-10 17:06:35,572 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2023-02-10 17:06:41,598 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "2023-02-10 17:06:46,619 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2023-02-10 17:06:53,660 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2023-02-10 17:06:59,686 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "2023-02-10 17:07:03,706 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "2023-02-10 17:07:07,722 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "2023-02-10 17:07:13,746 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2023-02-10 17:07:17,769 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "2023-02-10 17:07:19,783 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2023-02-10 17:07:24,808 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2023-02-10 17:07:26,815 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "2023-02-10 17:07:31,834 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2023-02-10 17:07:33,842 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2023-02-10 17:07:37,860 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "2023-02-10 17:07:43,888 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2023-02-10 17:07:49,910 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "2023-02-10 17:07:53,924 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2023-02-10 17:07:54,927 INFO mapreduce.Job:  map 27% reduce 2%\n",
      "2023-02-10 17:07:56,935 INFO mapreduce.Job:  map 28% reduce 2%\n",
      "2023-02-10 17:07:59,946 INFO mapreduce.Job:  map 29% reduce 2%\n",
      "2023-02-10 17:08:01,954 INFO mapreduce.Job:  map 30% reduce 2%\n",
      "2023-02-10 17:08:06,974 INFO mapreduce.Job:  map 30% reduce 3%\n",
      "2023-02-10 17:08:07,978 INFO mapreduce.Job:  map 31% reduce 3%\n",
      "2023-02-10 17:08:15,001 INFO mapreduce.Job:  map 32% reduce 3%\n",
      "2023-02-10 17:08:19,015 INFO mapreduce.Job:  map 33% reduce 3%\n",
      "2023-02-10 17:08:21,022 INFO mapreduce.Job:  map 34% reduce 3%\n",
      "2023-02-10 17:08:25,034 INFO mapreduce.Job:  map 34% reduce 4%\n",
      "2023-02-10 17:08:27,044 INFO mapreduce.Job:  map 35% reduce 4%\n",
      "2023-02-10 17:08:30,055 INFO mapreduce.Job:  map 36% reduce 4%\n",
      "2023-02-10 17:08:33,064 INFO mapreduce.Job:  map 37% reduce 4%\n",
      "2023-02-10 17:08:34,067 INFO mapreduce.Job:  map 37% reduce 7%\n",
      "2023-02-10 17:08:42,095 INFO mapreduce.Job:  map 38% reduce 7%\n",
      "2023-02-10 17:08:45,104 INFO mapreduce.Job:  map 39% reduce 7%\n",
      "2023-02-10 17:08:51,123 INFO mapreduce.Job:  map 40% reduce 7%\n",
      "2023-02-10 17:08:55,135 INFO mapreduce.Job:  map 40% reduce 8%\n",
      "2023-02-10 17:08:57,143 INFO mapreduce.Job:  map 41% reduce 8%\n",
      "2023-02-10 17:09:03,167 INFO mapreduce.Job:  map 42% reduce 8%\n",
      "2023-02-10 17:09:04,171 INFO mapreduce.Job:  map 43% reduce 8%\n",
      "2023-02-10 17:09:07,182 INFO mapreduce.Job:  map 43% reduce 9%\n",
      "2023-02-10 17:09:17,215 INFO mapreduce.Job:  map 44% reduce 9%\n",
      "2023-02-10 17:09:21,227 INFO mapreduce.Job:  map 45% reduce 9%\n",
      "2023-02-10 17:09:23,233 INFO mapreduce.Job:  map 46% reduce 9%\n",
      "2023-02-10 17:09:31,257 INFO mapreduce.Job:  map 47% reduce 9%\n",
      "2023-02-10 17:09:35,275 INFO mapreduce.Job:  map 48% reduce 9%\n",
      "2023-02-10 17:09:45,303 INFO mapreduce.Job:  map 49% reduce 9%\n",
      "2023-02-10 17:09:48,311 INFO mapreduce.Job:  map 50% reduce 9%\n",
      "2023-02-10 17:09:49,315 INFO mapreduce.Job:  map 50% reduce 10%\n",
      "2023-02-10 17:09:54,330 INFO mapreduce.Job:  map 51% reduce 10%\n",
      "2023-02-10 17:09:59,344 INFO mapreduce.Job:  map 52% reduce 10%\n",
      "2023-02-10 17:10:02,352 INFO mapreduce.Job:  map 53% reduce 10%\n",
      "2023-02-10 17:10:06,364 INFO mapreduce.Job:  map 54% reduce 10%\n",
      "2023-02-10 17:10:07,367 INFO mapreduce.Job:  map 54% reduce 11%\n",
      "2023-02-10 17:10:10,377 INFO mapreduce.Job:  map 54% reduce 12%\n",
      "2023-02-10 17:10:11,380 INFO mapreduce.Job:  map 54% reduce 17%\n",
      "2023-02-10 17:10:13,386 INFO mapreduce.Job:  map 54% reduce 18%\n",
      "2023-02-10 17:10:22,415 INFO mapreduce.Job:  map 55% reduce 18%\n",
      "2023-02-10 17:10:23,417 INFO mapreduce.Job:  map 56% reduce 18%\n",
      "2023-02-10 17:10:28,430 INFO mapreduce.Job:  map 57% reduce 18%\n",
      "2023-02-10 17:10:34,448 INFO mapreduce.Job:  map 58% reduce 18%\n",
      "2023-02-10 17:10:41,467 INFO mapreduce.Job:  map 59% reduce 18%\n",
      "2023-02-10 17:10:47,483 INFO mapreduce.Job:  map 60% reduce 18%\n",
      "2023-02-10 17:10:56,506 INFO mapreduce.Job:  map 61% reduce 18%\n",
      "2023-02-10 17:10:59,518 INFO mapreduce.Job:  map 62% reduce 18%\n",
      "2023-02-10 17:11:05,535 INFO mapreduce.Job:  map 63% reduce 18%\n",
      "2023-02-10 17:11:07,542 INFO mapreduce.Job:  map 63% reduce 19%\n",
      "2023-02-10 17:11:08,545 INFO mapreduce.Job:  map 64% reduce 19%\n",
      "2023-02-10 17:11:12,554 INFO mapreduce.Job:  map 64% reduce 20%\n",
      "2023-02-10 17:11:13,558 INFO mapreduce.Job:  map 64% reduce 21%\n",
      "2023-02-10 17:11:21,592 INFO mapreduce.Job:  map 65% reduce 21%\n",
      "2023-02-10 17:11:24,599 INFO mapreduce.Job:  map 66% reduce 21%\n",
      "2023-02-10 17:11:27,608 INFO mapreduce.Job:  map 67% reduce 21%\n",
      "2023-02-10 17:11:31,618 INFO mapreduce.Job:  map 68% reduce 21%\n",
      "2023-02-10 17:11:39,638 INFO mapreduce.Job:  map 69% reduce 21%\n",
      "2023-02-10 17:11:43,648 INFO mapreduce.Job:  map 70% reduce 21%\n",
      "2023-02-10 17:11:46,656 INFO mapreduce.Job:  map 70% reduce 22%\n",
      "2023-02-10 17:11:49,664 INFO mapreduce.Job:  map 71% reduce 22%\n",
      "2023-02-10 17:11:55,680 INFO mapreduce.Job:  map 72% reduce 22%\n",
      "2023-02-10 17:11:58,689 INFO mapreduce.Job:  map 72% reduce 23%\n",
      "2023-02-10 17:11:59,692 INFO mapreduce.Job:  map 73% reduce 23%\n",
      "2023-02-10 17:12:01,697 INFO mapreduce.Job:  map 73% reduce 24%\n",
      "2023-02-10 17:12:11,729 INFO mapreduce.Job:  map 74% reduce 24%\n",
      "2023-02-10 17:12:16,748 INFO mapreduce.Job:  map 75% reduce 24%\n",
      "2023-02-10 17:12:23,782 INFO mapreduce.Job:  map 76% reduce 24%\n",
      "2023-02-10 17:12:28,796 INFO mapreduce.Job:  map 77% reduce 24%\n",
      "2023-02-10 17:12:31,804 INFO mapreduce.Job:  map 77% reduce 25%\n",
      "2023-02-10 17:12:40,832 INFO mapreduce.Job:  map 78% reduce 25%\n",
      "2023-02-10 17:12:46,849 INFO mapreduce.Job:  map 79% reduce 25%\n",
      "2023-02-10 17:12:49,858 INFO mapreduce.Job:  map 80% reduce 25%\n",
      "2023-02-10 17:12:55,874 INFO mapreduce.Job:  map 81% reduce 25%\n",
      "2023-02-10 17:13:00,886 INFO mapreduce.Job:  map 81% reduce 26%\n",
      "2023-02-10 17:13:01,889 INFO mapreduce.Job:  map 82% reduce 26%\n",
      "2023-02-10 17:13:04,898 INFO mapreduce.Job:  map 82% reduce 27%\n",
      "2023-02-10 17:13:12,924 INFO mapreduce.Job:  map 83% reduce 27%\n",
      "2023-02-10 17:13:17,937 INFO mapreduce.Job:  map 84% reduce 27%\n",
      "2023-02-10 17:13:21,947 INFO mapreduce.Job:  map 85% reduce 27%\n",
      "2023-02-10 17:13:23,951 INFO mapreduce.Job:  map 86% reduce 27%\n",
      "2023-02-10 17:13:30,969 INFO mapreduce.Job:  map 87% reduce 27%\n",
      "2023-02-10 17:13:35,981 INFO mapreduce.Job:  map 88% reduce 27%\n",
      "2023-02-10 17:13:40,993 INFO mapreduce.Job:  map 88% reduce 28%\n",
      "2023-02-10 17:13:41,996 INFO mapreduce.Job:  map 89% reduce 28%\n",
      "2023-02-10 17:13:45,004 INFO mapreduce.Job:  map 90% reduce 28%\n",
      "2023-02-10 17:13:49,013 INFO mapreduce.Job:  map 90% reduce 29%\n",
      "2023-02-10 17:13:53,023 INFO mapreduce.Job:  map 90% reduce 30%\n",
      "2023-02-10 17:13:54,026 INFO mapreduce.Job:  map 91% reduce 30%\n",
      "2023-02-10 17:14:00,039 INFO mapreduce.Job:  map 92% reduce 30%\n",
      "2023-02-10 17:14:06,055 INFO mapreduce.Job:  map 93% reduce 30%\n",
      "2023-02-10 17:14:08,060 INFO mapreduce.Job:  map 94% reduce 30%\n",
      "2023-02-10 17:14:12,069 INFO mapreduce.Job:  map 95% reduce 30%\n",
      "2023-02-10 17:14:18,084 INFO mapreduce.Job:  map 97% reduce 30%\n",
      "2023-02-10 17:14:19,086 INFO mapreduce.Job:  map 97% reduce 31%\n",
      "2023-02-10 17:14:23,096 INFO mapreduce.Job:  map 98% reduce 31%\n",
      "2023-02-10 17:14:30,112 INFO mapreduce.Job:  map 99% reduce 31%\n",
      "2023-02-10 17:14:35,128 INFO mapreduce.Job:  map 100% reduce 32%\n",
      "2023-02-10 17:14:42,145 INFO mapreduce.Job:  map 100% reduce 34%\n",
      "2023-02-10 17:14:43,147 INFO mapreduce.Job:  map 100% reduce 35%\n",
      "2023-02-10 17:14:44,150 INFO mapreduce.Job:  map 100% reduce 36%\n",
      "2023-02-10 17:14:48,160 INFO mapreduce.Job:  map 100% reduce 38%\n",
      "2023-02-10 17:14:49,163 INFO mapreduce.Job:  map 100% reduce 40%\n",
      "2023-02-10 17:14:50,165 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "2023-02-10 17:14:54,174 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "2023-02-10 17:14:55,177 INFO mapreduce.Job:  map 100% reduce 45%\n",
      "2023-02-10 17:14:56,180 INFO mapreduce.Job:  map 100% reduce 47%\n",
      "2023-02-10 17:15:00,190 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "2023-02-10 17:15:01,193 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2023-02-10 17:15:02,195 INFO mapreduce.Job:  map 100% reduce 52%\n",
      "2023-02-10 17:15:06,204 INFO mapreduce.Job:  map 100% reduce 54%\n",
      "2023-02-10 17:15:07,206 INFO mapreduce.Job:  map 100% reduce 56%\n",
      "2023-02-10 17:15:08,208 INFO mapreduce.Job:  map 100% reduce 57%\n",
      "2023-02-10 17:15:12,218 INFO mapreduce.Job:  map 100% reduce 59%\n",
      "2023-02-10 17:15:13,221 INFO mapreduce.Job:  map 100% reduce 61%\n",
      "2023-02-10 17:15:14,223 INFO mapreduce.Job:  map 100% reduce 63%\n",
      "2023-02-10 17:15:18,232 INFO mapreduce.Job:  map 100% reduce 64%\n",
      "2023-02-10 17:15:19,235 INFO mapreduce.Job:  map 100% reduce 66%\n",
      "2023-02-10 17:15:20,239 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2023-02-10 17:15:43,295 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "2023-02-10 17:16:08,356 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "2023-02-10 17:16:35,419 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "2023-02-10 17:17:00,480 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "2023-02-10 17:17:26,542 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "2023-02-10 17:17:53,605 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "2023-02-10 17:18:18,663 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "2023-02-10 17:18:44,733 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2023-02-10 17:19:11,798 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "2023-02-10 17:19:36,858 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "2023-02-10 17:20:02,917 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "2023-02-10 17:20:29,979 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "2023-02-10 17:20:55,044 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "2023-02-10 17:21:24,117 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "2023-02-10 17:21:49,175 INFO mapreduce.Job:  map 100% reduce 82%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/hw2/result || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"longest-session\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/longest_session.py \\\n",
    "-mapper \"python3 longest_session.py map\" \\\n",
    "-reducer \"python3 longest_session.py reduce\" \\\n",
    "-input /user/hw2/data/ \\\n",
    "-output /user/hw2/result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3a2884ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t540\r\n",
      "100\t199\r\n",
      "1000\t3022\r\n",
      "10000\t964\r\n",
      "100000\t0\r\n",
      "1000000\t0\r\n",
      "1000001\t937\r\n",
      "1000002\t0\r\n",
      "1000003\t1023\r\n",
      "1000005\t0\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/hw2/result/* | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ce748756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hw2/top10\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob6669817881849552092.jar tmpDir=null\n",
      "2023-02-11 09:03:01,637 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-11 09:03:01,864 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-11 09:03:01,898 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:8032\n",
      "2023-02-11 09:03:01,899 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net/10.128.0.8:10200\n",
      "2023-02-11 09:03:02,109 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1676026733147_0005\n",
      "2023-02-11 09:03:02,405 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2023-02-11 09:03:02,481 INFO mapreduce.JobSubmitter: number of splits:21\n",
      "2023-02-11 09:03:02,644 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1676026733147_0005\n",
      "2023-02-11 09:03:02,647 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-11 09:03:02,832 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-02-11 09:03:02,833 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-02-11 09:03:02,911 INFO impl.YarnClientImpl: Submitted application application_1676026733147_0005\n",
      "2023-02-11 09:03:02,974 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-4doa8g36b59o741x.mdb.yandexcloud.net:8088/proxy/application_1676026733147_0005/\n",
      "2023-02-11 09:03:02,976 INFO mapreduce.Job: Running job: job_1676026733147_0005\n",
      "2023-02-11 09:03:08,048 INFO mapreduce.Job: Job job_1676026733147_0005 running in uber mode : false\n",
      "2023-02-11 09:03:08,049 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-11 09:03:18,136 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2023-02-11 09:03:19,142 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "2023-02-11 09:03:21,158 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-02-11 09:03:27,191 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2023-02-11 09:03:28,197 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "2023-02-11 09:03:30,211 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "2023-02-11 09:03:31,215 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2023-02-11 09:03:32,220 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "2023-02-11 09:03:36,238 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2023-02-11 09:03:37,244 INFO mapreduce.Job:  map 76% reduce 0%\n",
      "2023-02-11 09:03:38,249 INFO mapreduce.Job:  map 76% reduce 24%\n",
      "2023-02-11 09:03:39,254 INFO mapreduce.Job:  map 81% reduce 24%\n",
      "2023-02-11 09:03:42,268 INFO mapreduce.Job:  map 90% reduce 24%\n",
      "2023-02-11 09:03:43,272 INFO mapreduce.Job:  map 100% reduce 24%\n",
      "2023-02-11 09:03:44,276 INFO mapreduce.Job:  map 100% reduce 35%\n",
      "2023-02-11 09:03:50,299 INFO mapreduce.Job:  map 100% reduce 52%\n",
      "2023-02-11 09:03:56,320 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "2023-02-11 09:04:02,343 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "2023-02-11 09:04:04,354 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-11 09:04:05,363 INFO mapreduce.Job: Job job_1676026733147_0005 completed successfully\n",
      "2023-02-11 09:04:05,440 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32524307\n",
      "\t\tFILE: Number of bytes written=73869563\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=107354699\n",
      "\t\tHDFS: Number of bytes written=146\n",
      "\t\tHDFS: Number of read operations=68\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=22\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=19\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=534498\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=126717\n",
      "\t\tTotal time spent by all map tasks (ms)=178166\n",
      "\t\tTotal time spent by all reduce tasks (ms)=42239\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=178166\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=42239\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=547325952\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=129758208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=9025781\n",
      "\t\tMap output records=9025781\n",
      "\t\tMap output bytes=115274491\n",
      "\t\tMap output materialized bytes=36018572\n",
      "\t\tInput split bytes=2898\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8596971\n",
      "\t\tReduce shuffle bytes=36018572\n",
      "\t\tReduce input records=9025781\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=18051562\n",
      "\t\tShuffled Maps =21\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=21\n",
      "\t\tGC time elapsed (ms)=3858\n",
      "\t\tCPU time spent (ms)=130780\n",
      "\t\tPhysical memory (bytes) snapshot=13298089984\n",
      "\t\tVirtual memory (bytes) snapshot=95461638144\n",
      "\t\tTotal committed heap usage (bytes)=13359382528\n",
      "\t\tPeak Map Physical memory (bytes)=679485440\n",
      "\t\tPeak Map Virtual memory (bytes)=4343435264\n",
      "\t\tPeak Reduce Physical memory (bytes)=3529023488\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6818476032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=107351801\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=146\n",
      "2023-02-11 09:04:05,440 INFO streaming.StreamJob: Output directory: /user/hw2/top10/\n",
      "CPU times: user 990 ms, sys: 184 ms, total: 1.17 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/hw2/top10/\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python top10.py map\" \\\n",
    "-reducer \"python top10.py reduce\" \\\n",
    "-input /user/hw2/result/ \\\n",
    "-output /user/hw2/top10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f4ade7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113291\t329463\r\n",
      "1113291\t329327\r\n",
      "1113291\t329327\r\n",
      "1203081\t109500\r\n",
      "1203081\t109474\r\n",
      "1203081\t109362\r\n",
      "4263912\t74135\r\n",
      "4263912\t73660\r\n",
      "4263912\t71505\r\n",
      "1987990\t70543\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/hw2/top10/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d38fe67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2023-02-11 09:04 /user/hw2/top10/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        146 2023-02-11 09:04 /user/hw2/top10/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/hw2/top10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "02cf8f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-11 09:08:44,113 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-11 09:08:44,196 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-11 09:08:44,196 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-02-11 09:08:46,908 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-11 09:08:46,909 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-11 09:08:46,909 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/hw2/top10/part-00000 s3a://lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "932ea440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-11 09:08:49,870 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-11 09:08:49,954 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-11 09:08:49,954 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 2 items\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        157 2023-01-23 09:16 s3a://lsml-sasha-bakalova-data/mhw1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        146 2023-02-11 09:08 s3a://lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt\n",
      "2023-02-11 09:08:51,438 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-11 09:08:51,438 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-11 09:08:51,439 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls s3a://lsml-sasha-bakalova-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7ceca",
   "metadata": {},
   "source": [
    "## [link](https://storage.yandexcloud.net/lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt) to object storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8003b",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca033a",
   "metadata": {},
   "source": [
    "## Downloading data (we need other tables now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c016fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n",
      "Downloading avito-context-ad-clicks.zip to /home/ubuntu\n",
      "100%|██████████████████████████████████████| 17.8G/17.8G [07:27<00:00, 59.9MB/s]\n",
      "100%|██████████████████████████████████████| 17.8G/17.8G [07:27<00:00, 42.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle competitions download -c avito-context-ad-clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df17718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 56 not upgraded.\n",
      "Need to get 168 kB of archives.\n",
      "After this operation, 593 kB of additional disk space will be used.\n",
      "Get:1 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 unzip amd64 6.0-25ubuntu1.1 [168 kB]\n",
      "Fetched 168 kB in 0s (7,462 kB/s)\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 126845 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-25ubuntu1.1_amd64.deb ...\n",
      "Unpacking unzip (6.0-25ubuntu1.1) ...\n",
      "Setting up unzip (6.0-25ubuntu1.1) ...\n",
      "Processing triggers for mime-support (3.64ubuntu1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "! sudo apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58626132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  avito-context-ad-clicks.zip\n",
      "replace AdsInfo.tsv.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: AdsInfo.tsv.7z          \n",
      "replace Category.tsv.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: Category.tsv.7z         \n",
      "replace Location.tsv.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: Location.tsv.7z         \n",
      "replace PhoneRequestsStream.tsv.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: PhoneRequestsStream.tsv.7z  \n",
      "replace SearchInfo.tsv.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: SearchInfo.tsv.7z       \n",
      "replace UserInfo.tsv.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: UserInfo.tsv.7z         \n",
      "replace VisitsStream.tsv.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: VisitsStream.tsv.7z     \n",
      "  inflating: database.sqlite.7z      \n",
      "  inflating: sampleSubmission.csv.7z  \n",
      "  inflating: sampleSubmission_HistCTR.csv.7z  \n",
      "  inflating: testSearchStream.tsv.7z  \n",
      "  inflating: trainSearchStream.tsv.7z  \n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! yes | unzip avito-context-ad-clicks.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7868de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel Xeon Processor (Icelake) (606A0),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 2132222657 bytes (2034 MiB)\n",
      "\n",
      "Extracting archive: VisitsStream.tsv.7z\n",
      "--\n",
      "Path = VisitsStream.tsv.7z\n",
      "Type = 7z\n",
      "Physical Size = 2132222657\n",
      "Headers Size = 136\n",
      "Method = LZMA:24\n",
      "Solid = -\n",
      "Blocks = 1\n",
      "\n",
      "      0% - VisitsStream.ts                        1% - VisitsStream.ts                        2% - VisitsStream.ts                        3% - VisitsStream.ts                        4% - VisitsStream.ts                        5% - VisitsStream.ts                        6% - VisitsStream.ts                        7% - VisitsStream.ts                        8% - VisitsStream.ts                        9% - VisitsStream.ts                       10% - VisitsStream.ts                       11% - VisitsStream.ts                       12% - VisitsStream.ts                       13% - VisitsStream.ts                       14% - VisitsStream.ts                       15% - VisitsStream.ts                       16% - VisitsStream.ts                       17% - VisitsStream.ts                       18% - VisitsStream.ts                       19% - VisitsStream.ts                       20% - VisitsStream.ts                       21% - VisitsStream.ts                       22% - VisitsStream.ts                       23% - VisitsStream.ts                       24% - VisitsStream.ts                       25% - VisitsStream.ts                       26% - VisitsStream.ts                       27% - VisitsStream.ts                       28% - VisitsStream.ts                       29% - VisitsStream.ts                       30% - VisitsStream.ts                       31% - VisitsStream.ts                       32% - VisitsStream.ts                       33% - VisitsStream.ts                       34% - VisitsStream.ts                       35% - VisitsStream.ts                       36% - VisitsStream.ts                       37% - VisitsStream.ts                       38% - VisitsStream.ts                       39% - VisitsStream.ts                       40% - VisitsStream.ts                       41% - VisitsStream.ts                       42% - VisitsStream.ts                       43% - VisitsStream.ts                       44% - VisitsStream.ts                       45% - VisitsStream.ts                       46% - VisitsStream.ts                       47% - VisitsStream.ts                       48% - VisitsStream.ts                       49% - VisitsStream.ts                       50% - VisitsStream.ts                       51% - VisitsStream.ts                       52% - VisitsStream.ts                       53% - VisitsStream.ts                       54% - VisitsStream.ts                       55% - VisitsStream.ts                       56% - VisitsStream.ts                       57% - VisitsStream.ts                       58% - VisitsStream.ts                       59% - VisitsStream.ts                       60% - VisitsStream.ts                       61% - VisitsStream.ts                       62% - VisitsStream.ts                       63% - VisitsStream.ts                       64% - VisitsStream.ts                       65% - VisitsStream.ts                       66% - VisitsStream.ts                       67% - VisitsStream.ts                       68% - VisitsStream.ts                       69% - VisitsStream.ts                       70% - VisitsStream.ts                       71% - VisitsStream.ts                       72% - VisitsStream.ts                       73% - VisitsStream.ts                       74% - VisitsStream.ts                       75% - VisitsStream.ts                       76% - VisitsStream.ts                       77% - VisitsStream.ts                       78% - VisitsStream.ts                       79% - VisitsStream.ts                       80% - VisitsStream.ts                       81% - VisitsStream.ts                       82% - VisitsStream.ts                       83% - VisitsStream.ts                       84% - VisitsStream.ts                       85% - VisitsStream.ts                       86% - VisitsStream.ts                       87% - VisitsStream.ts                       88% - VisitsStream.ts                       89% - VisitsStream.ts                       90% - VisitsStream.ts                       91% - VisitsStream.ts                       92% - VisitsStream.ts                       93% - VisitsStream.ts                       94% - VisitsStream.ts                       95% - VisitsStream.ts                       96% - VisitsStream.ts                       97% - VisitsStream.ts                       98% - VisitsStream.ts                       99% - VisitsStream.ts                      100%    Everything is Ok\n",
      "\n",
      "Size:       13180996392\n",
      "Compressed: 2132222657\n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel Xeon Processor (Icelake) (606A0),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 2277264844 bytes (2172 MiB)\n",
      "\n",
      "Extracting archive: trainSearchStream.tsv.7z\n",
      "--\n",
      "Path = trainSearchStream.tsv.7z\n",
      "Type = 7z\n",
      "Physical Size = 2277264844\n",
      "Headers Size = 146\n",
      "Method = LZMA:24\n",
      "Solid = -\n",
      "Blocks = 1\n",
      "\n",
      "      0% - trainSearchStream.t                            1% - trainSearchStream.t                            2% - trainSearchStream.t                            3% - trainSearchStream.t                            4% - trainSearchStream.t                            5% - trainSearchStream.t                            6% - trainSearchStream.t                            7% - trainSearchStream.t                            8% - trainSearchStream.t                            9% - trainSearchStream.t                           10% - trainSearchStream.t                           11% - trainSearchStream.t                           12% - trainSearchStream.t                           13% - trainSearchStream.t                           14% - trainSearchStream.t                           15% - trainSearchStream.t                           16% - trainSearchStream.t                           17% - trainSearchStream.t                           18% - trainSearchStream.t                           19% - trainSearchStream.t                           20% - trainSearchStream.t                           21% - trainSearchStream.t                           22% - trainSearchStream.t                           23% - trainSearchStream.t                           24% - trainSearchStream.t                           25% - trainSearchStream.t                           26% - trainSearchStream.t                           27% - trainSearchStream.t                           28% - trainSearchStream.t                           29% - trainSearchStream.t                           30% - trainSearchStream.t                           31% - trainSearchStream.t                           32% - trainSearchStream.t                           33% - trainSearchStream.t                           34% - trainSearchStream.t                           35% - trainSearchStream.t                           36% - trainSearchStream.t                           37% - trainSearchStream.t                           38% - trainSearchStream.t                           39% - trainSearchStream.t                           40% - trainSearchStream.t                           41% - trainSearchStream.t                           42% - trainSearchStream.t                           43% - trainSearchStream.t                           44% - trainSearchStream.t                           45% - trainSearchStream.t                           46% - trainSearchStream.t                           47% - trainSearchStream.t                           48% - trainSearchStream.t                           49% - trainSearchStream.t                           50% - trainSearchStream.t                           51% - trainSearchStream.t                           52% - trainSearchStream.t                           53% - trainSearchStream.t                           54% - trainSearchStream.t                           55% - trainSearchStream.t                           56% - trainSearchStream.t                           57% - trainSearchStream.t                           58% - trainSearchStream.t                           59% - trainSearchStream.t                           60% - trainSearchStream.t                           61% - trainSearchStream.t                           62% - trainSearchStream.t                           63% - trainSearchStream.t                           64% - trainSearchStream.t                           65% - trainSearchStream.t                           66% - trainSearchStream.t                           67% - trainSearchStream.t                           68% - trainSearchStream.t                           69% - trainSearchStream.t                           70% - trainSearchStream.t                           71% - trainSearchStream.t                           72% - trainSearchStream.t                           73% - trainSearchStream.t                           74% - trainSearchStream.t                           75% - trainSearchStream.t                           76% - trainSearchStream.t                           77% - trainSearchStream.t                           78% - trainSearchStream.t                           79% - trainSearchStream.t                           80% - trainSearchStream.t                           81% - trainSearchStream.t                           82% - trainSearchStream.t                           83% - trainSearchStream.t                           84% - trainSearchStream.t                           85% - trainSearchStream.t                           86% - trainSearchStream.t                           87% - trainSearchStream.t                           88% - trainSearchStream.t                           89% - trainSearchStream.t                           90% - trainSearchStream.t                           91% - trainSearchStream.t                           92% - trainSearchStream.t                           93% - trainSearchStream.t                           94% - trainSearchStream.t                           95% - trainSearchStream.t                           96% - trainSearchStream.t                           97% - trainSearchStream.t                           98% - trainSearchStream.t                           99% - trainSearchStream.t                          100%    Everything is Ok\n",
      "\n",
      "Size:       11023566785\n",
      "Compressed: 2277264844\n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel Xeon Processor (Icelake) (606A0),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 12681881 bytes (13 MiB)\n",
      "\n",
      "Extracting archive: UserInfo.tsv.7z\n",
      "--\n",
      "Path = UserInfo.tsv.7z\n",
      "Type = 7z\n",
      "Physical Size = 12681881\n",
      "Headers Size = 126\n",
      "Method = LZMA:24\n",
      "Solid = -\n",
      "Blocks = 1\n",
      "\n",
      "     12% - UserInfo.ts                   28% - UserInfo.ts                   44% - UserInfo.ts                   60% - UserInfo.ts                   80% - UserInfo.ts                   99% - UserInfo.ts                  Everything is Ok\n",
      "\n",
      "Size:       104614699\n",
      "Compressed: 12681881\n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel Xeon Processor (Icelake) (606A0),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 1813431506 bytes (1730 MiB)\n",
      "\n",
      "Extracting archive: SearchInfo.tsv.7z\n",
      "--\n",
      "Path = SearchInfo.tsv.7z\n",
      "Type = 7z\n",
      "Physical Size = 1813431506\n",
      "Headers Size = 132\n",
      "Method = LZMA:24\n",
      "Solid = -\n",
      "Blocks = 1\n",
      "\n",
      "      0% - SearchInfo.ts                      1% - SearchInfo.ts                      2% - SearchInfo.ts                      3% - SearchInfo.ts                      4% - SearchInfo.ts                      5% - SearchInfo.ts                      6% - SearchInfo.ts                      7% - SearchInfo.ts                      8% - SearchInfo.ts                      9% - SearchInfo.ts                     10% - SearchInfo.ts                     11% - SearchInfo.ts                     12% - SearchInfo.ts                     13% - SearchInfo.ts                     14% - SearchInfo.ts                     15% - SearchInfo.ts                     16% - SearchInfo.ts                     17% - SearchInfo.ts                     18% - SearchInfo.ts                     19% - SearchInfo.ts                     20% - SearchInfo.ts                     21% - SearchInfo.ts                     22% - SearchInfo.ts                     23% - SearchInfo.ts                     24% - SearchInfo.ts                     25% - SearchInfo.ts                     26% - SearchInfo.ts                     27% - SearchInfo.ts                     28% - SearchInfo.ts                     29% - SearchInfo.ts                     30% - SearchInfo.ts                     31% - SearchInfo.ts                     32% - SearchInfo.ts                     33% - SearchInfo.ts                     34% - SearchInfo.ts                     35% - SearchInfo.ts                     36% - SearchInfo.ts                     37% - SearchInfo.ts                     38% - SearchInfo.ts                     39% - SearchInfo.ts                     40% - SearchInfo.ts                     41% - SearchInfo.ts                     42% - SearchInfo.ts                     43% - SearchInfo.ts                     44% - SearchInfo.ts                     45% - SearchInfo.ts                     46% - SearchInfo.ts                     47% - SearchInfo.ts                     48% - SearchInfo.ts                     49% - SearchInfo.ts                     50% - SearchInfo.ts                     51% - SearchInfo.ts                     52% - SearchInfo.ts                     53% - SearchInfo.ts                     54% - SearchInfo.ts                     55% - SearchInfo.ts                     56% - SearchInfo.ts                     57% - SearchInfo.ts                     58% - SearchInfo.ts                     59% - SearchInfo.ts                     60% - SearchInfo.ts                     61% - SearchInfo.ts                     62% - SearchInfo.ts                     63% - SearchInfo.ts                     64% - SearchInfo.ts                     65% - SearchInfo.ts                     66% - SearchInfo.ts                     67% - SearchInfo.ts                     68% - SearchInfo.ts                     69% - SearchInfo.ts                     70% - SearchInfo.ts                     71% - SearchInfo.ts                     72% - SearchInfo.ts                     73% - SearchInfo.ts                     74% - SearchInfo.ts                     75% - SearchInfo.ts                     76% - SearchInfo.ts                     77% - SearchInfo.ts                     78% - SearchInfo.ts                     79% - SearchInfo.ts                     80% - SearchInfo.ts                     81% - SearchInfo.ts                     82% - SearchInfo.ts                     83% - SearchInfo.ts                     84% - SearchInfo.ts                     85% - SearchInfo.ts                     86% - SearchInfo.ts                     87% - SearchInfo.ts                     88% - SearchInfo.ts                     89% - SearchInfo.ts                     90% - SearchInfo.ts                     91% - SearchInfo.ts                     92% - SearchInfo.ts                     93% - SearchInfo.ts                     94% - SearchInfo.ts                     95% - SearchInfo.ts                     96% - SearchInfo.ts                     97% - SearchInfo.ts                     98% - SearchInfo.ts                     99% - SearchInfo.ts                    100%    Everything is Ok\n",
      "\n",
      "Size:       9469373867\n",
      "Compressed: 1813431506\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p hw2folder2 && 7z x VisitsStream.tsv.7z -ohw2folder2/\n",
    "! 7z x trainSearchStream.tsv.7z -ohw2folder2/\n",
    "! 7z x UserInfo.tsv.7z -ohw2folder2/\n",
    "! 7z x SearchInfo.tsv.7z -ohw2folder2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ca9f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a971cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2c256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"lsml-app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910543e",
   "metadata": {},
   "source": [
    "### Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e98802",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put hw2folder2/SearchInfo.tsv /user/hw2/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82db7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info = sc.textFile(\"/user/hw2/data/SearchInfo.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f5e907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_filters_with_dots(searchparams):\n",
    "    if searchparams == \"\" or searchparams == 'SearchParams':\n",
    "        return []\n",
    "    pattern = re.compile(r\"[0-9]+:\")\n",
    "    return pattern.findall(s)\n",
    "\n",
    "result = (\n",
    "    search_info\n",
    "    .map(lambda row : row.split(\"\\t\")[-1])\n",
    "    .flatMap(get_filters_with_dots)\n",
    "    .map(lambda x : (int(x[:-1]), 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .takeOrdered(10, lambda x: -x[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9b90852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(797, 53562267),\n",
       " (5, 53562267),\n",
       " (799, 53562267),\n",
       " (800, 53562267),\n",
       " (801, 53562267),\n",
       " (709, 53562267)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5676f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"hw2/subtask1.txt\", \"w\") as file:\n",
    "    file.write('\\n'.join(list(map(lambda x : str(x[0]), result))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29ba9069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797\r\n",
      "5\r\n",
      "799\r\n",
      "800\r\n",
      "801\r\n",
      "709"
     ]
    }
   ],
   "source": [
    "! cat \"hw2/subtask1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "939cde10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-11 23:49:29,770 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-11 23:49:30,119 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-11 23:49:30,120 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-02-11 23:49:33,559 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-11 23:49:33,559 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-11 23:49:33,559 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put hw2/subtask1.txt s3a://lsml-sasha-bakalova-data/mhw2-subtask1-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4edadb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-11 23:49:38,240 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-11 23:49:38,317 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-11 23:49:38,317 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 3 items\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        157 2023-01-23 09:16 s3a://lsml-sasha-bakalova-data/mhw1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        146 2023-02-11 09:08 s3a://lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu         21 2023-02-11 23:49 s3a://lsml-sasha-bakalova-data/mhw2-subtask1-output.txt\n",
      "2023-02-11 23:49:39,790 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-11 23:49:39,790 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-11 23:49:39,790 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls s3a://lsml-sasha-bakalova-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e023d4",
   "metadata": {},
   "source": [
    "### [link](https://storage.yandexcloud.net/lsml-sasha-bakalova-data/mhw2-subtask1-output.txt) to object storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418eb63c",
   "metadata": {},
   "source": [
    "### Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a277a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put hw2folder2/trainSearchStream.tsv /user/hw2/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d00d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51745797",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info_df = se.read.csv(\"/user/hw2/data/SearchInfo.tsv\", sep=\"\\t\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5c07c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+-------+--------------+-----------+----------+----------+--------------------+\n",
      "|SearchID|          SearchDate|   IPID| UserID|IsUserLoggedOn|SearchQuery|LocationID|CategoryID|        SearchParams|\n",
      "+--------+--------------------+-------+-------+--------------+-----------+----------+----------+--------------------+\n",
      "|       1|2015-05-18 19:54:...|1717090|3640266|             0|       null|      1729|         5|                null|\n",
      "|       2|2015-05-12 14:21:...|1731568| 769304|             0|       null|       697|        50|                null|\n",
      "|       3|2015-05-12 07:09:...| 793143| 640089|             0|       null|      1261|        12|                null|\n",
      "|       4|2015-05-10 18:11:...| 898705|3573776|             0|       null|      3960|        22|{83:'Обувь', 175:...|\n",
      "|       5|2015-04-25 13:04:...|2009707| 320674|             0|       null|       547|         1|                null|\n",
      "|       6|2015-05-07 16:49:...|1658456|1665156|             0|       null|       926|        27|                null|\n",
      "|       7|2015-05-14 23:07:...|1849117|3434614|             0|       null|        44|    500001|                null|\n",
      "|       8|2015-05-09 09:10:...| 572585| 905821|             0|       null|      2694|        12|                null|\n",
      "|       9|2015-05-02 20:14:...|  77162|1106541|             0|       null|       576|         4|                null|\n",
      "|      10|2015-05-15 17:38:...|1371072|3310798|             0|       null|      1110|        12|                null|\n",
      "|      11|2015-05-17 12:01:...|  67356|3113486|             1|       null|      3953|        22|  {175:'Аксессуары'}|\n",
      "|      12|2015-05-13 11:54:...|1305191| 632714|             0|       null|      1729|         4|      {156:'Горные'}|\n",
      "|      13|2015-04-26 21:34:...|1473851| 638475|             0|       null|      3960|        38|{45:'Кровати, див...|\n",
      "|      14|2015-04-28 01:30:...| 473331|2962661|             1|       null|      3961|        38|{45:'Кровати, див...|\n",
      "|      15|2015-05-06 11:23:...|1117012|2515153|             0|       null|      3725|        34|                null|\n",
      "|      16|2015-05-10 21:08:...|2022127|1259151|             0|       null|      2114|        34|{5:'Шины, диски и...|\n",
      "|      17|2015-05-03 14:12:...| 697982| 943133|             1|       null|      3960|        37|                null|\n",
      "|      19|2015-05-12 10:14:...|2080435|2909452|             0|       null|      1261|        60|{110:'Верхняя оде...|\n",
      "|      20|2015-05-16 14:38:...| 486408|4280401|             0|       null|      3679|         4|                null|\n",
      "|      21|2015-05-14 17:27:...|1279103|3419273|             0|       null|       576|        38|{45:'Подставки и ...|\n",
      "+--------+--------------------+-------+-------+--------------+-----------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_info_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "246d64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_stream_df = se.read.csv(\"/user/hw2/data/trainSearchStream.tsv\", sep=\"\\t\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e93e4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+----------+--------+-------+\n",
      "|SearchID|    AdID|Position|ObjectType| HistCTR|IsClick|\n",
      "+--------+--------+--------+----------+--------+-------+\n",
      "|       2|11441863|       1|         3|0.001804|      0|\n",
      "|       2|22968355|       7|         3|0.004723|      0|\n",
      "|       3|  212187|       7|         3|0.029701|      0|\n",
      "|       3|34084553|       1|         3|  0.0043|      0|\n",
      "|       3|36256251|       2|         2|    null|   null|\n",
      "|       4| 2073399|       6|         1|    null|   null|\n",
      "|       4| 6046052|       7|         1|    null|   null|\n",
      "|       4|17544913|       8|         1|    null|   null|\n",
      "|       4|20653823|       1|         3|0.003049|      0|\n",
      "|       4|24129570|       2|         2|    null|   null|\n",
      "|       5|  530202|       6|         1|    null|   null|\n",
      "|       5|11219482|       1|         3|0.043897|      0|\n",
      "|       5|13375896|       7|         3|0.001563|      0|\n",
      "|       5|30074367|       2|         2|    null|   null|\n",
      "|       5|33506654|       8|         1|    null|   null|\n",
      "|       6| 6303835|       1|         3|0.007044|      0|\n",
      "|       6|28312593|       7|         3|0.002199|      0|\n",
      "|       8|24728248|       1|         3|0.003647|      0|\n",
      "|       8|27598775|       7|         3| 0.03137|      0|\n",
      "|       9| 1723280|       6|         1|    null|   null|\n",
      "+--------+--------+--------+----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_stream_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6999720",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info_df.registerTempTable('search_info')\n",
    "search_stream_df.registerTempTable('search_stream') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09375b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = se.sql(\"\"\"\n",
    "    SELECT SearchQuery\n",
    "    FROM search_info si\n",
    "        inner join search_stream ss on ss.SearchID = si.SearchID\n",
    "    WHERE ss.IsClick = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72976b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         SearchQuery|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|                null|\n",
      "|             монопод|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|сварочный аппарат бу|\n",
      "|                null|\n",
      "|     iphone 5s новый|\n",
      "|            ваз 2110|\n",
      "|               туфли|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2db51fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def transform_query(query):\n",
    "    if query is None:\n",
    "        return []\n",
    "    return [\"\".join([i if i not in string.punctuation else \"\" for i in w]) for w in query.lower().split()]\n",
    "\n",
    "result = (\n",
    "    words.rdd\n",
    "    .map(lambda x : x.SearchQuery)\n",
    "    .flatMap(transform_query)\n",
    "    .map(lambda x : (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .takeOrdered(10, lambda x: -x[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c30e505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('бу', 12599),\n",
       " ('велосипед', 12510),\n",
       " ('iphone', 7442),\n",
       " ('для', 5984),\n",
       " ('диван', 5716),\n",
       " ('на', 4905),\n",
       " ('платье', 4465),\n",
       " ('коляска', 3785),\n",
       " ('велосипеды', 3780),\n",
       " ('самокат', 3447)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3fb4c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бу\t12599\r\n",
      "велосипед\t12510\r\n",
      "iphone\t7442\r\n",
      "для\t5984\r\n",
      "диван\t5716\r\n",
      "на\t4905\r\n",
      "платье\t4465\r\n",
      "коляска\t3785\r\n",
      "велосипеды\t3780\r\n",
      "самокат\t3447"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"hw2/subtask2.txt\", \"w\") as file:\n",
    "    file.write('\\n'.join(list(map(lambda x : f\"{x[0]}\\t{x[1]}\", result))))\n",
    "! cat \"hw2/subtask2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4cc064a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-11 23:53:10,575 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-11 23:53:10,659 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-11 23:53:10,660 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-02-11 23:53:12,678 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-11 23:53:12,678 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-11 23:53:12,678 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put hw2/subtask2.txt s3a://lsml-sasha-bakalova-data/mhw2-subtask2-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "515a2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-11 23:53:13,879 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-11 23:53:13,950 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-11 23:53:13,950 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 4 items\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        157 2023-01-23 09:16 s3a://lsml-sasha-bakalova-data/mhw1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        146 2023-02-11 09:08 s3a://lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu         21 2023-02-11 23:49 s3a://lsml-sasha-bakalova-data/mhw2-subtask1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        169 2023-02-11 23:53 s3a://lsml-sasha-bakalova-data/mhw2-subtask2-output.txt\n",
      "2023-02-11 23:53:15,445 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-11 23:53:15,445 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-11 23:53:15,445 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls s3a://lsml-sasha-bakalova-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7a8d9",
   "metadata": {},
   "source": [
    "### [link](https://storage.yandexcloud.net/lsml-sasha-bakalova-data/mhw2-subtask2-output.txt) to object storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355b6a2",
   "metadata": {},
   "source": [
    "### Task 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10a4235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/hw2/data/VisitsStream.tsv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put hw2folder2/VisitsStream.tsv /user/hw2/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c278012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_stream = sc.textFile(\"/user/hw2/data/VisitsStream.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3490f27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59703\\t1259356\\t469877\\t2015-04-25 00:00:00.0',\n",
       " '154389\\t1846749\\t27252551\\t2015-04-25 00:00:00.0',\n",
       " '218628\\t2108380\\t31685325\\t2015-04-25 00:00:00.0',\n",
       " '231535\\t837110\\t18827716\\t2015-04-25 00:00:00.0',\n",
       " '282306\\t1654210\\t29363673\\t2015-04-25 00:00:00.0',\n",
       " '295068\\t601505\\t588324\\t2015-04-25 00:00:00.0',\n",
       " '501897\\t158476\\t4103261\\t2015-04-25 00:00:00.0',\n",
       " '655394\\t631692\\t9860544\\t2015-04-25 00:00:00.0',\n",
       " '765603\\t804403\\t29475627\\t2015-04-25 00:00:00.0',\n",
       " '790289\\t121085\\t23309988\\t2015-04-25 00:00:00.0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# already got rid of the header in the first task\n",
    "visits_stream.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72807527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of days in the dataset\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_date(row):\n",
    "    date = datetime.strptime(row.split(\"\\t\")[-1], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return (date.year, date.month, date.day)\n",
    "\n",
    "(\n",
    "    visits_stream\n",
    "    .map(get_date)\n",
    "    .distinct()\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54256fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TOTAL_NUM_DAYS = 26 # result of the previous calculation\n",
    "\n",
    "def get_user_and_date(row):\n",
    "    date = datetime.strptime(row.split(\"\\t\")[-1], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return (row.split(\"\\t\")[0], f\"{date.year} {date.month} {date.day}\")\n",
    "\n",
    "(\n",
    "    visits_stream\n",
    "    .map(get_user_and_date)\n",
    "    .distinct()\n",
    "    .map(lambda x : (x[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x : x[1] == TOTAL_NUM_DAYS)\n",
    "    .saveAsTextFile(\"hw2/everyday_users.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2deecfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3003794',\n",
       " '981692',\n",
       " '3230519',\n",
       " '3296369',\n",
       " '1247227',\n",
       " '1243244',\n",
       " '1407121',\n",
       " '2266007',\n",
       " '1212571',\n",
       " '2539404']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everyday_users = sc.textFile(\"hw2/everyday_users.txt\").map(lambda x : eval(x)[0])\n",
    "everyday_users.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bec8587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = everyday_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8be29392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "157d3ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"hw2/subtask3.txt\", \"w\") as file:\n",
    "    file.write(str(result))\n",
    "! cat \"hw2/subtask3.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee19a4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-12 14:22:04,372 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-12 14:22:04,472 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-12 14:22:04,472 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-02-12 14:22:06,855 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-12 14:22:06,855 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-12 14:22:06,855 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put hw2/subtask3.txt s3a://lsml-sasha-bakalova-data/mhw2-subtask3-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84714699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-12 14:22:08,018 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-12 14:22:08,105 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-12 14:22:08,105 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 5 items\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        157 2023-01-23 09:16 s3a://lsml-sasha-bakalova-data/mhw1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        146 2023-02-11 09:08 s3a://lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu         21 2023-02-11 23:49 s3a://lsml-sasha-bakalova-data/mhw2-subtask1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        169 2023-02-11 23:53 s3a://lsml-sasha-bakalova-data/mhw2-subtask2-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu          3 2023-02-12 14:22 s3a://lsml-sasha-bakalova-data/mhw2-subtask3-output.txt\n",
      "2023-02-12 14:22:09,607 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-12 14:22:09,607 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-12 14:22:09,607 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls s3a://lsml-sasha-bakalova-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f4b66",
   "metadata": {},
   "source": [
    "### [link](https://storage.yandexcloud.net/lsml-sasha-bakalova-data/mhw2-subtask3-output.txt) to object storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7134eaa",
   "metadata": {},
   "source": [
    "### Task 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d54525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is okay, because we have only 807 users in this list\n",
    "everyday_users_list = everyday_users.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f8075d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59703\\t1259356\\t469877\\t2015-04-25 00:00:00.0',\n",
       " '154389\\t1846749\\t27252551\\t2015-04-25 00:00:00.0',\n",
       " '218628\\t2108380\\t31685325\\t2015-04-25 00:00:00.0']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visits_stream.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d740273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_date_and_user(row):\n",
    "    date = datetime.strptime(row.split(\"\\t\")[-1], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return (f\"{date.year}-{date.month}-{date.day}\", row.split(\"\\t\")[0])\n",
    "\n",
    "def swap_key_value(kv):\n",
    "    return (kv[1], kv[0])\n",
    "\n",
    "\n",
    "# we can call collect because there are only 26 distinct days in the dataset\n",
    "result = (\n",
    "    visits_stream\n",
    "    .map(get_date_and_user)\n",
    "    .filter(lambda x : str(x[1]) not in everyday_users_list)\n",
    "    .distinct()\n",
    "    .map(lambda x : (x[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(swap_key_value)\n",
    "    .sortByKey(ascending=False)\n",
    "    .map(swap_key_value)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03914897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2015-5-12', 973248),\n",
       " ('2015-5-13', 847446),\n",
       " ('2015-5-14', 745904),\n",
       " ('2015-5-6', 733027),\n",
       " ('2015-5-5', 731624),\n",
       " ('2015-5-7', 729708),\n",
       " ('2015-5-11', 706240),\n",
       " ('2015-4-27', 676433),\n",
       " ('2015-5-8', 664982),\n",
       " ('2015-4-28', 655791),\n",
       " ('2015-4-29', 641148),\n",
       " ('2015-5-15', 633132),\n",
       " ('2015-5-4', 607205),\n",
       " ('2015-4-30', 596224),\n",
       " ('2015-5-10', 562151),\n",
       " ('2015-4-26', 540051),\n",
       " ('2015-5-3', 511687),\n",
       " ('2015-4-25', 498069),\n",
       " ('2015-5-16', 493577),\n",
       " ('2015-5-18', 488250),\n",
       " ('2015-5-17', 480968),\n",
       " ('2015-5-2', 468010),\n",
       " ('2015-5-1', 452800),\n",
       " ('2015-5-9', 450135),\n",
       " ('2015-5-19', 367777),\n",
       " ('2015-5-20', 184972)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da01884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-5-12\t973248\r\n",
      "2015-5-13\t847446\r\n",
      "2015-5-14\t745904\r\n",
      "2015-5-6\t733027\r\n",
      "2015-5-5\t731624\r\n",
      "2015-5-7\t729708\r\n",
      "2015-5-11\t706240\r\n",
      "2015-4-27\t676433\r\n",
      "2015-5-8\t664982\r\n",
      "2015-4-28\t655791\r\n",
      "2015-4-29\t641148\r\n",
      "2015-5-15\t633132\r\n",
      "2015-5-4\t607205\r\n",
      "2015-4-30\t596224\r\n",
      "2015-5-10\t562151\r\n",
      "2015-4-26\t540051\r\n",
      "2015-5-3\t511687\r\n",
      "2015-4-25\t498069\r\n",
      "2015-5-16\t493577\r\n",
      "2015-5-18\t488250\r\n",
      "2015-5-17\t480968\r\n",
      "2015-5-2\t468010\r\n",
      "2015-5-1\t452800\r\n",
      "2015-5-9\t450135\r\n",
      "2015-5-19\t367777\r\n",
      "2015-5-20\t184972"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"hw2/subtask4.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(map(lambda x : f\"{x[0]}\\t{x[1]}\", result)))\n",
    "! cat \"hw2/subtask4.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f73fa87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-12 16:03:20,149 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-12 16:03:20,230 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-12 16:03:20,230 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-02-12 16:03:22,067 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-12 16:03:22,067 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-12 16:03:22,067 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put hw2/subtask4.txt s3a://lsml-sasha-bakalova-data/mhw2-subtask4-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ff5a8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-12 16:03:23,271 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-12 16:03:23,346 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-12 16:03:23,346 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 6 items\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        157 2023-01-23 09:16 s3a://lsml-sasha-bakalova-data/mhw1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        146 2023-02-11 09:08 s3a://lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu         21 2023-02-11 23:49 s3a://lsml-sasha-bakalova-data/mhw2-subtask1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        169 2023-02-11 23:53 s3a://lsml-sasha-bakalova-data/mhw2-subtask2-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu          3 2023-02-12 14:22 s3a://lsml-sasha-bakalova-data/mhw2-subtask3-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        432 2023-02-12 16:03 s3a://lsml-sasha-bakalova-data/mhw2-subtask4-output.txt\n",
      "2023-02-12 16:03:24,830 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-12 16:03:24,831 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-12 16:03:24,831 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls s3a://lsml-sasha-bakalova-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab57d1",
   "metadata": {},
   "source": [
    "### [link](https://storage.yandexcloud.net/lsml-sasha-bakalova-data/mhw2-subtask4-output.txt) to object storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d550607",
   "metadata": {},
   "source": [
    "### Task 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212adaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info = sc.textFile(\"/user/hw2/data/SearchInfo.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26b3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put hw2folder2/UserInfo.tsv /user/hw2/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91420b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = sc.textFile(\"/user/hw2/data/UserInfo.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15f65f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID\tUserAgentID\tUserAgentOSID\tUserDeviceID\tUserAgentFamilyID\n",
      "1\t44073\t30\t2019\t9\n",
      "2\t12505\t20\t2014\t85\n",
      "3\t24256\t20\t2014\t64\n",
      "4\t57133\t20\t2014\t25\n",
      "5\t57133\t20\t2014\t25\n",
      "6\t57201\t20\t2014\t85\n",
      "7\t22293\t20\t2014\t25\n",
      "8\t19983\t35\t3870\t63\n",
      "9\t12505\t20\t2014\t85\n",
      "cat: write error: Broken pipe\n",
      "SearchID\tSearchDate\tIPID\tUserID\tIsUserLoggedOn\tSearchQuery\tLocationID\tCategoryID\tSearchParams\n",
      "1\t2015-05-18 19:54:32.0\t1717090\t3640266\t0\t\t1729\t5\t\n",
      "2\t2015-05-12 14:21:28.0\t1731568\t769304\t0\t\t697\t50\t\n",
      "3\t2015-05-12 07:09:42.0\t793143\t640089\t0\t\t1261\t12\t\n",
      "4\t2015-05-10 18:11:01.0\t898705\t3573776\t0\t\t3960\t22\t{83:'Обувь', 175:'Женская одежда', 88:'38'}\n",
      "5\t2015-04-25 13:04:09.0\t2009707\t320674\t0\t\t547\t1\t\n",
      "6\t2015-05-07 16:49:15.0\t1658456\t1665156\t0\t\t926\t27\t\n",
      "7\t2015-05-14 23:07:27.0\t1849117\t3434614\t0\t\t44\t500001\t\n",
      "8\t2015-05-09 09:10:06.0\t572585\t905821\t0\t\t2694\t12\t\n",
      "9\t2015-05-02 20:14:15.0\t77162\t1106541\t0\t\t576\t4\t\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! cat hw2folder2/UserInfo.tsv | head\n",
    "! cat hw2folder2/SearchInfo.tsv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "630fe35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def get_relevant_rows_user_info_user_first(row):\n",
    "    splitted = row.split(\"\\t\")\n",
    "    if splitted[0] == \"UserID\":\n",
    "        return []\n",
    "    return [(splitted[0], splitted[2])]\n",
    "\n",
    "def get_relevant_rows_search_info_user_first(row):\n",
    "    splitted = row.split(\"\\t\")\n",
    "    if splitted[0] == \"SearchID\":\n",
    "        return []\n",
    "    return [(splitted[3], splitted[7])]\n",
    "\n",
    "def get_relevant_rows_user_info(row):\n",
    "    splitted = row.split(\"\\t\")\n",
    "    if splitted[0] == \"UserID\":\n",
    "        return []\n",
    "    return [(splitted[2], splitted[0])]\n",
    "\n",
    "def get_relevant_rows_search_info(row):\n",
    "    splitted = row.split(\"\\t\")\n",
    "    if splitted[0] == \"SearchID\":\n",
    "        return []\n",
    "    return [(splitted[7], splitted[3])]\n",
    "\n",
    "def get_category_to_count(row):\n",
    "    splitted = row.split(\"\\t\")\n",
    "    if splitted[0] == \"SearchID\":\n",
    "        return []\n",
    "    return [(splitted[7], 1)]\n",
    "\n",
    "def get_userosid_to_count(row):\n",
    "    splitted = row.split(\"\\t\")\n",
    "    if splitted[0] == \"UserID\":\n",
    "        return []\n",
    "    return [(splitted[2], 1)]\n",
    "\n",
    "def userid_first(row):\n",
    "    key1, (userid, cnt) = row\n",
    "    return userid, (key1, cnt)\n",
    "\n",
    "def get_correlation(cnt_1, cnt_2, cnt_both, cnt_total):\n",
    "    up = cnt_both - 2 * cnt_1 * cnt_2 / cnt_total + cnt_1 * cnt_2 / (cnt_total ** 2)\n",
    "    down = sqrt((cnt_1 - cnt_1 ** 2 / cnt_total) * (cnt_2 - cnt_2 ** 2 / cnt_total))\n",
    "    return up / down\n",
    "\n",
    "DATASET_LEN = (\n",
    "    search_info\n",
    "        .flatMap(get_relevant_rows_search_info_user_first)\n",
    "        .join(\n",
    "            user_info\n",
    "            .flatMap(get_relevant_rows_user_info_user_first)\n",
    "        )\n",
    "    .count()\n",
    ")\n",
    "\n",
    "result = ( # (catid, userosid)\n",
    "    search_info\n",
    "    .flatMap(get_relevant_rows_search_info)\n",
    "    .join(\n",
    "        search_info\n",
    "        .flatMap(get_category_to_count)\n",
    "        .reduceByKey(lambda a, b : a + b)\n",
    "    )\n",
    "    .map(userid_first)\n",
    "    .join(\n",
    "        user_info\n",
    "        .flatMap(get_relevant_rows_user_info)\n",
    "        .join(\n",
    "            user_info\n",
    "            .flatMap(get_userosid_to_count)\n",
    "            .reduceByKey(lambda a, b : a + b)\n",
    "        )\n",
    "        .map(userid_first)\n",
    "    )\n",
    "    .map(lambda row: ((row[1][0][0], row[1][1][0]), (row[1][0][1], row[1][1][1])))# (cat, userosid), (#cat, #userosid)\n",
    "    .join(\n",
    "        search_info\n",
    "        .flatMap(get_relevant_rows_search_info_user_first)\n",
    "        .join(\n",
    "            user_info\n",
    "            .flatMap(get_relevant_rows_user_info_user_first)\n",
    "        )\n",
    "        .map(lambda row : (row[1], 1))\n",
    "        .reduceByKey(lambda a, b : a + b)\n",
    "    )\n",
    "    .map(lambda row : (row[0], (row[1][0][0], row[1][0][1], row[1][1]))) # (cat, userosid), (#cat, #userosid, #cat_and_userosid)\n",
    "    .map(lambda row : (row[0], get_correlation(*row[1], DATASET_LEN)))\n",
    "    .distinct()\n",
    "    .takeOrdered(10, lambda x : -x[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86ebf3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('34', '20'), 1.3631589353908604),\n",
       " (('22', '20'), 1.2870800179055524),\n",
       " (('38', '20'), 1.2185713707280852),\n",
       " (('12', '20'), 1.1098459756153582),\n",
       " (('60', '20'), 1.0088773311553494),\n",
       " (('47', '20'), 1.0058198235530058),\n",
       " (('0', '20'), 0.9646020438376409),\n",
       " (('41', '20'), 0.8445054700158525),\n",
       " (('50', '20'), 0.8124688986998632),\n",
       " (('26', '20'), 0.7245170642697342)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed508079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\t34\t1.3631589353908604\r\n",
      "20\t22\t1.2870800179055524\r\n",
      "20\t38\t1.2185713707280852\r\n",
      "20\t12\t1.1098459756153582\r\n",
      "20\t60\t1.0088773311553494\r\n",
      "20\t47\t1.0058198235530058\r\n",
      "20\t0\t0.9646020438376409\r\n",
      "20\t41\t0.8445054700158525\r\n",
      "20\t50\t0.8124688986998632\r\n",
      "20\t26\t0.7245170642697342"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"hw2/subtask5.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(map(lambda x : f\"{x[0][1]}\\t{x[0][0]}\\t{x[1]}\", result)))\n",
    "! cat \"hw2/subtask5.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f37d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-13 13:00:38,508 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-13 13:00:38,587 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-13 13:00:38,587 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-02-13 13:00:41,560 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-13 13:00:41,560 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-13 13:00:41,560 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put hw2/subtask5.txt s3a://lsml-sasha-bakalova-data/mhw2-subtask5-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d92e6cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-13 13:00:42,888 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-02-13 13:00:42,973 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-02-13 13:00:42,973 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 7 items\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        157 2023-01-23 09:16 s3a://lsml-sasha-bakalova-data/mhw1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        146 2023-02-11 09:08 s3a://lsml-sasha-bakalova-data/mhw2-mapreduce-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu         21 2023-02-11 23:49 s3a://lsml-sasha-bakalova-data/mhw2-subtask1-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        169 2023-02-11 23:53 s3a://lsml-sasha-bakalova-data/mhw2-subtask2-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu          3 2023-02-12 14:22 s3a://lsml-sasha-bakalova-data/mhw2-subtask3-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        432 2023-02-12 16:03 s3a://lsml-sasha-bakalova-data/mhw2-subtask4-output.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu        248 2023-02-13 13:00 s3a://lsml-sasha-bakalova-data/mhw2-subtask5-output.txt\n",
      "2023-02-13 13:00:44,575 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-02-13 13:00:44,575 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-02-13 13:00:44,575 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls s3a://lsml-sasha-bakalova-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b4a43",
   "metadata": {},
   "source": [
    "### [link](https://storage.yandexcloud.net/lsml-sasha-bakalova-data/mhw2-subtask5-output.txt) to object storage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
